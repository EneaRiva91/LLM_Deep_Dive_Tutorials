{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Chapter 2 Tutorial: Understanding LLMs and Pre-training\n",
        "\n",
        "In this tutorial, we will explore the mechanics of LLM architectures, with an emphasis on the differences between masked models and causal models. In the first section, we'll examine some existing pretrained models to understand how they produce their outputs. Once we've demonstrated how LLM's are able to do what they do, we will then run an abbreviated training loop to provide a glimpse into the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p66qIkzd7wz"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGW64FX-d7wz"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece,torch]\n",
        "!pip install apache_beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForMaskedLM,\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGc8Si8CBbuI"
      },
      "source": [
        "## Understanding Masked LM's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99o28B2EGVh3"
      },
      "outputs": [],
      "source": [
        "## The first model we will look at is BERT, which is trained with masked tokens. As an example,\n",
        "## the text below masks the word \"box\" from a well-known movie quote.\n",
        "\n",
        "text = \"Life is like a [MASK] of chocolates.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVii0JgSBKpU"
      },
      "outputs": [],
      "source": [
        "## We'll now see how BERT is able to predict the missing word. We can use HuggingFace to load\n",
        "## a copy of the pretrained model and tokenizer.\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWKpRFEFA2Oz"
      },
      "outputs": [],
      "source": [
        "## Next, we'll feed our example text into the tokenizer.\n",
        "\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "print('input_ids:', encoded_input['input_ids'])\n",
        "print('attention_mask:', encoded_input['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o84muqUuLiwC"
      },
      "outputs": [],
      "source": [
        "## input_ids represents the tokenized output. Each integer can be mapped back to the corresponding string.\n",
        "\n",
        "print(tokenizer.decode([7967]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ut5R3IYbxxd"
      },
      "outputs": [],
      "source": [
        "## The model will then receive the output of the tokenizer. We can look at the BERT model to see exactly how\n",
        "## it was constructed and what the outputs will be like.\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Xv2ujRAch4"
      },
      "outputs": [],
      "source": [
        "## The model starts with an embedding of each of the 30,522 possible tokens into 768 dimensions, which at this\n",
        "## point is simply a representation of each token without any additional information about their relationships\n",
        "## to one another in the text. Then the encoder attention blocks are applied, updating the embeddings such that\n",
        "## they now encode each token's contribution to the chunk of text and interactions with other tokens. Notably,\n",
        "## this includes the masked tokens as well. The final stage is the language model head, which takes the embeddings\n",
        "## from the masked positions back to 30,522 dimensions. Each index of this final vector corresponds to the\n",
        "## probability that the token in that position would be the correct choice to fill the mask.\n",
        "\n",
        "\n",
        "model_output = model(**encoded_input)\n",
        "output = model_output[\"logits\"]\n",
        "\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hDXB-x5tzrh"
      },
      "outputs": [],
      "source": [
        "tokens = encoded_input['input_ids'][0].tolist()\n",
        "masked_index = tokens.index(tokenizer.mask_token_id)\n",
        "logits = output[0, masked_index, :]\n",
        "\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaRYd_aVjeoR"
      },
      "outputs": [],
      "source": [
        "probs = logits.softmax(dim=-1)\n",
        "values, predictions = probs.topk(5)\n",
        "sequence = tokenizer.decode(predictions)\n",
        "\n",
        "print('Top 5 predictions:', sequence)\n",
        "print(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vVQ6Cqhuq9t"
      },
      "source": [
        "Printing the top 5 predictions and their respective scores, we see that BERT accurately chooses \"box\" as the most likely replacement for the mask token.\n",
        "\n",
        "## Understanding Causal LM's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azFAE8TVkPXv"
      },
      "outputs": [],
      "source": [
        "## We now repeat a similar exercise with the causal LLM GPT-2. This model generates\n",
        "## text following an input, instead of replacing a mask within the text.\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TwmbE0bk1mC"
      },
      "outputs": [],
      "source": [
        "## We can examine the model again, noting the similarities to BERT. An embedding, 12 attention blocks,\n",
        "## and a linear transformation bringing the output back to the size of the tokenizer. The tokenizer is\n",
        "## different from BERT so we see we have more tokens this time.\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwUCWU6TwTqZ"
      },
      "outputs": [],
      "source": [
        "## We'll use a different text example, since this model works by producing tokens sequentially\n",
        "## rather than filling a mask.\n",
        "\n",
        "text = \"Swimming at the beach is\"\n",
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua8YctwekqSm"
      },
      "outputs": [],
      "source": [
        "## After applying the model, the information needed to predict the next token is represented by\n",
        "## the last token. So we can access that vector by the index -1.\n",
        "\n",
        "output = model(**model_inputs)\n",
        "next_token_logits = output.logits[:, -1, :]\n",
        "next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "print(next_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH05O28PnLW9"
      },
      "outputs": [],
      "source": [
        "## Now add the new token to the end of the text, and feed all of it back to the model to continue\n",
        "## predicting more tokens.\n",
        "\n",
        "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
        "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
        "print(model_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ja9Mk0DnpBe"
      },
      "outputs": [],
      "source": [
        "## Here's what we have so far. The model added the word 'a' to the input text.\n",
        "\n",
        "print(tokenizer.decode(model_inputs['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ygamSIoMKh"
      },
      "outputs": [],
      "source": [
        "## Repeating all the previous steps, we then add the word 'great'.\n",
        "\n",
        "output = model(**model_inputs)\n",
        "next_token_logits = output.logits[:, -1, :]\n",
        "next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
        "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
        "print(tokenizer.decode(model_inputs['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9QIw--gIrh8"
      },
      "outputs": [],
      "source": [
        "## HuggingFace automates this iterative process. We'll use the quicker approach to finish our sentence.\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_length=20, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(output_generate[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL2rmSMVpkEu"
      },
      "source": [
        "## Pre-training a GPT-2 model from scratch\n",
        "\n",
        "Next we'll train a GPT-2 model from scratch using English Wikipedia data. Note that we're only using a tiny subset of the data to demonstrate that the model is capable of learning. The exact same approach could be followed on the full dataset to train a more functional model, but that would require a lot of compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTW0UmOJd7w2"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
        "ds_shuffle = dataset['train'].shuffle()\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_shuffle.select(range(50)),\n",
        "        \"valid\": ds_shuffle.select(range(50, 100))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qbgOvH3d7w2"
      },
      "outputs": [],
      "source": [
        "print(raw_datasets['train'][0]['text'][:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HRmdiqbd7w3"
      },
      "outputs": [],
      "source": [
        "## We'll tokenize the text, setting the context size to 128 and thus breaking each document into chunks of 128 tokens.\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][:2][\"text\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWvoQ3-td7w3"
      },
      "outputs": [],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfu1oJeQYiLa"
      },
      "source": [
        "Now we can set up the HuggingFace Trainer as follows. Since we're using such a small dataset, we'll need lots of epochs for the model to make progress because all of the parameters are randomly initialized at the outset. Typically, most LLM's are trained for only one epoch and more diverse examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jf0Le4Yd7w3"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_OKDaPMd7w3"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex3MHkued7w4"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"wiki-gpt2\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIj0n1fbd7w4"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2avxzYR5U8Y"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIkCLEFRc3h6"
      },
      "source": [
        "The training loss is low by the end, which means the model should perform very well on training examples it has seen. It does not generalize well to the validation set of course, since we deliberately overfit on a small train set.\n",
        "\n",
        "We can confirm with a couple of examples that were seen in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-cxmRLLsGgB"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids'][:16])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbk_WBhMrj_F"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "print(model_inputs['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipgkkqn4sEAp"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
        "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
        "output_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHUbsd-hxdTL"
      },
      "outputs": [],
      "source": [
        "sequence = tokenizer.decode(output_generate[0])\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l04YrQLeghA5"
      },
      "source": [
        "The model should do quite well at reciting text after seeing it so many times. We can be convinced that the tokenizer, model architecture, and training objective are well-suited to learning Wikipedia data. For comparison, we'll try this model on text from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFK5FC57hwS9"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.decode(tokenized_datasets[\"valid\"][0]['input_ids'][:32])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SlX_sfHfdTB"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
        "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
        "sequence = tokenizer.decode(output_generate[0])\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FassINSdfs9U"
      },
      "outputs": [],
      "source": [
        "raw_datasets['valid'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ivQP0XciLl1"
      },
      "source": [
        "As expected, our model is completely confused this time. We'd need to train for much longer, and on much more diverse data, before we would have a model that can sensibly complete prompts it has never seen before. This is precisely why pre-training is such an important and powerful technique. If we had to train on all of Wikipedia for every NLP application to achieve optimal performance, it would be prohibitively expensive. But there's no need to do that when we can share and reuse existing pre-trained models as we did in the first part of this tutorial."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
