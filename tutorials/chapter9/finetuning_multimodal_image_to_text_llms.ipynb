{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4r8kcaVzmMA"
   },
   "source": [
    "# Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0aaO3dOjKae",
    "outputId": "518dcba2-0639-45cf-e2e8-abd404cbfde9"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets bitsandbytes sentencepiece\n",
    "!pip install -q accelerate loralib peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LHbBkpBkij8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from transformers import (\n",
    "    IdeficsForVisionText2Text,\n",
    "    AutoProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvmVvVFozA6q"
   },
   "source": [
    "# Load the IDEFICS model with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_Of_9cMk1u7",
    "outputId": "6162e88c-4cc0-4e2c-e281-615037c9dcc6"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "c9231fcbe0ef4929ad14d552cd0da13d",
      "88184606f0354c69989df4c8c6b2173e",
      "a936eb6dde0345e083dab5afb712aa00",
      "cbefb02bb1f54d3ba09931230546e6d2",
      "5613366936064871a05bb0761ea84193",
      "3e8c929b279148dca4a0a8ac4d420bed",
      "ee253a22120b42abb2db14978d6b70b1",
      "06b706b02e9b4815b821a428d46f3b1c",
      "477f4aa8712449fc911bf1febf50616a",
      "9d5dd69ff225431a921daad46e7d3078",
      "266b1b9893a0492e825841d6453d3a33"
     ]
    },
    "id": "QDSfuT9CkkXS",
    "outputId": "9806d2b5-b513-46c7-e1a5-a1c7ec187be7"
   },
   "outputs": [],
   "source": [
    "## Load in IDEFICS with 4bit quantization\n",
    "checkpoint = \"HuggingFaceM4/idefics-9b\"\n",
    "\n",
    "# Here we skip some special modules that can't be quantized properly\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\"],\n",
    ")\n",
    "\n",
    "## Load the IDEFICS processor, which encodes images and tokenizes text\n",
    "processor = AutoProcessor.from_pretrained(checkpoint, use_auth_token=True)\n",
    "\n",
    "## Download the model from HF\n",
    "model = IdeficsForVisionText2Text.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNVhg1LuAqV2"
   },
   "outputs": [],
   "source": [
    "## Create a simple script to predict on images\n",
    "\n",
    "def check_inference(model, processor, prompts, max_new_tokens=50):\n",
    "    ## Collect tokenizer\n",
    "    tokenizer = processor.tokenizer\n",
    "\n",
    "    ## Remove IDEFICS tags from text for tokenizing\n",
    "    bad_words = [\"<image>\", \"<fake_token_around_image>\"]\n",
    "    if len(bad_words) > 0:\n",
    "        bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids\n",
    "    eos_token = \"</s>\"\n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "    ## Process the prompt and generate outputs.\n",
    "    inputs = processor(prompts, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(**inputs, eos_token_id=[eos_token_id], bad_words_ids=bad_words_ids, max_new_tokens=max_new_tokens, early_stopping=True)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUGvzEKUzGoI"
   },
   "source": [
    "# Download the sports classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eqEoUlMviWt"
   },
   "outputs": [],
   "source": [
    "## Upload your kaggle.json credentials file. You can find this with the following steps:\n",
    "## 1) Create a Kaggle account\n",
    "## 2) Go to your Kaggle settings page.\n",
    "## 3) Scroll down to the API section, and \"Create New Token\". This will download a json credentials file.\n",
    "## 4) Upload the .json to the Colab notebook in /content/\n",
    "\n",
    "## Determine in the kaggle config folder exists. If not create it.\n",
    "\n",
    "if os.path.exists('~/.kaggle/'): ## This isn't working right\n",
    "    sys.exit()\n",
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jjyOAm2vqSZ",
    "outputId": "3740a9d8-9b54-41ae-aaab-02d5aa134804"
   },
   "outputs": [],
   "source": [
    "## Copy your cred file into the config folder and configure access.\n",
    "%cd /content/\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PohHiCVvri5",
    "outputId": "512ba6ef-8777-4ea2-f066-43cd59af69bc"
   },
   "outputs": [],
   "source": [
    "## If the data has not been downloaded, retrieve from kaggle\n",
    "!kaggle datasets download -d gpiosenka/sports-classification\n",
    "!unzip -q sports-classification.zip -d sports-classification\n",
    "\n",
    "## Format so that there are no spaces in sport name\n",
    "for folder in ('train','valid','test'):\n",
    "    path = f'./sports-classification/{folder}'\n",
    "    sports = os.listdir(path)\n",
    "    for sport in sports:\n",
    "        if ' ' in sport:\n",
    "            newsport = sport.replace(' ','_')\n",
    "            sport = sport.replace(' ','\\ ')\n",
    "            os.system(f'mv {path}/{sport} {path}/{newsport}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sI2p-tWqzPZo"
   },
   "source": [
    "# Format as Huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjUDIS2GzMo4"
   },
   "outputs": [],
   "source": [
    "## Routine to convert each image into a multimodal prompt asking which sport\n",
    "## is in the image.\n",
    "def process_data(batch):\n",
    "    prompts = []\n",
    "    for i in range(len(batch['files'])):\n",
    "        f, l = batch['files'][i], batch['labels'][i].replace('_',' ')\n",
    "        # We split the captions to avoid having very long examples, which would require more GPU ram during training\n",
    "        image = Image.open(f)\n",
    "        prompts.append(\n",
    "            [\n",
    "                image,\n",
    "                f\"Question: What sport is in this image? Answer: {l}. \",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    inputs = processor(prompts, return_tensors=\"pt\").to(device)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "## Routine to collect needed info about the dataset into a df.\n",
    "def create_dataset(folder):\n",
    "    files = [[folder+fs+'/'+f for f in os.listdir(folder+fs)] for fs in os.listdir(folder)]\n",
    "    files = [l for ls in files for l in ls]\n",
    "    labels = [tf.split('/')[-2] for tf in files]\n",
    "    df = pd.DataFrame({'files':files,'labels':labels})\n",
    "    return df\n",
    "\n",
    "\n",
    "## Load and prepare the dataset\n",
    "train_df = create_dataset('./sports-classification/train/')\n",
    "valid_df = create_dataset('./sports-classification/valid/')\n",
    "test_df = create_dataset('./sports-classification/test/')\n",
    "\n",
    "## Subsample of training data, include 10 examples of each sport\n",
    "all_sports = train_df.labels.drop_duplicates()\n",
    "out_indices = []\n",
    "for sport in all_sports:\n",
    "    indices = train_df[train_df.labels==sport].sample(50,random_state=7).index.tolist()\n",
    "    out_indices.extend(indices)\n",
    "train_df = train_df.iloc[out_indices].sample(frac=1,random_state=13).copy()\n",
    "\n",
    "## Create HF dataset and apply transformation\n",
    "sports_dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'valid': Dataset.from_pandas(valid_df),\n",
    "    'test': Dataset.from_pandas(test_df),\n",
    "})\n",
    "\n",
    "sports_dataset['train'].set_transform(process_data)\n",
    "sports_dataset['valid'].set_transform(process_data)\n",
    "sports_dataset['test'].set_transform(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8kcNv1izYpz",
    "outputId": "36551e53-07a5-4b4c-b478-024c84c211e7"
   },
   "outputs": [],
   "source": [
    "sports_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6XLzORPRBLI"
   },
   "source": [
    "# Sports classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mD0cy-0lH7u"
   },
   "source": [
    "## Zero-shot eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "gfGr9N8I1f08",
    "outputId": "2aa13f44-83f8-4cd1-ea1c-61f313e838ba"
   },
   "outputs": [],
   "source": [
    "## Test a single zero-shot example\n",
    "image = Image.open('./sports-classification/test/cricket/2.jpg')\n",
    "prompts = [\n",
    "    image,\n",
    "    \"Question: What sport is in this image? Answer:\",\n",
    "]\n",
    "display(image)\n",
    "print(check_inference(model, processor, prompts, max_new_tokens=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "rPwSnZwmAw2X",
    "outputId": "2378d399-e4c7-48b2-c390-7138ed3ddda1"
   },
   "outputs": [],
   "source": [
    "## Test a single zero-shot example\n",
    "image = Image.open('/content/sports-classification/test/tug_of_war/4.jpg')\n",
    "prompts = [\n",
    "    image,\n",
    "    \"Question: What sport is in this image? Answer:\",\n",
    "]\n",
    "display(image)\n",
    "print(check_inference(model, processor, prompts, max_new_tokens=10).replace('is in this','is in\\nthis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmE5AgiIlc4D",
    "outputId": "66fc56c9-966d-4f30-d6ed-566363b8ffa4"
   },
   "outputs": [],
   "source": [
    "## Lets predict on the full test set in this zero-shot approach and see what happens.\n",
    "out_labels = []\n",
    "for f, l in tqdm(test_df.to_numpy()):\n",
    "\n",
    "    image = Image.open(f)\n",
    "    prompts = [\n",
    "        # \"Instruction: provide an answer to the question. Use the image to answer.\\n\",\n",
    "        image,\n",
    "        \"Question: What sport is in this image? Answer:\",\n",
    "    ]\n",
    "    output = check_inference(model, processor, prompts, max_new_tokens=10)\n",
    "    out_labels.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DB7WKDYfoMZH"
   },
   "outputs": [],
   "source": [
    "test_df['predictions'] = out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQuaKlI8oVOj"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('./test_set_zero_shot_preds.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEKAk2iGqr2Q"
   },
   "source": [
    "## Fine-tune with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKOWtc9TtIHh",
    "outputId": "4e3ad467-422d-4785-ab21-8d5ca5a83b93"
   },
   "outputs": [],
   "source": [
    "model_name = checkpoint.split(\"/\")[1]\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xbVQCvgtIFT",
    "outputId": "ee07bd33-0c66-41dd-c807-30a719c41f96"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-sports\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNtuXTkxtIDB"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=sports_dataset['train'],\n",
    "    eval_dataset=sports_dataset['valid'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DwyYhF_ltIAz",
    "outputId": "133e47da-6e3d-4f37-b02a-3e98caf76d15"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQCtKB1atKsZ"
   },
   "outputs": [],
   "source": [
    "test_df = test_df[['files','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xObU1SE7tH7p",
    "outputId": "53f1c03b-fc7f-4b3f-e899-19d9f845d427"
   },
   "outputs": [],
   "source": [
    "## Predict on the full test set in this zero-shot approach and see what happens.\n",
    "ft_out_labels = []\n",
    "for f, l in tqdm(test_df.to_numpy()):\n",
    "\n",
    "    image = Image.open(f)\n",
    "    prompts = [\n",
    "        # \"Instruction: provide an answer to the question. Use the image to answer.\\n\",\n",
    "        image,\n",
    "        \"Question: What sport is in this image? Answer:\",\n",
    "    ]\n",
    "    output = check_inference(trainer.model, processor, prompts, max_new_tokens=15)\n",
    "    ft_out_labels.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNoEEafTHhl6"
   },
   "outputs": [],
   "source": [
    "test_df['predictions'] = ft_out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yssK0bEx1Mi"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('./test_set_qlora_preds.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5sskVdeKrh8"
   },
   "source": [
    "## Grade the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPV5B889yPPx"
   },
   "outputs": [],
   "source": [
    "## Read in the results\n",
    "df1 = pd.read_csv('./test_set_zero_shot_preds.csv')\n",
    "df2 = pd.read_csv('./test_set_qlora_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiB6VsalPdLD"
   },
   "outputs": [],
   "source": [
    "## Normalize the answers and compare against the dataset label.\n",
    "def norm_preds(p):\n",
    "    try: p = p.split('image? Answer: ')[1]\n",
    "    except:\n",
    "        return ''\n",
    "    p = p.split('.')[0]\n",
    "    p = p.lower()\n",
    "    return p\n",
    "\n",
    "df1['preds_norm'] = df1.predictions.apply(norm_preds)\n",
    "df2['preds_norm'] = df2.predictions.apply(norm_preds)\n",
    "\n",
    "df1['labels'] = df1.labels.apply(lambda x: x.replace('_',' '))\n",
    "df2['labels'] = df2.labels.apply(lambda x: x.replace('_',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLlaNAnBKn8Y",
    "outputId": "a37b7606-4a89-497e-a0bc-5375b3d4480b"
   },
   "outputs": [],
   "source": [
    "## How many labels were correctly predicted with zero-shot\n",
    "print(sum(df1.labels == df1.preds_norm),' / 500')\n",
    "\n",
    "## NOTE: loading with just LoRA instead of QLoRA increases score by 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8bkFiu0Kn55",
    "outputId": "d8b2a6c2-c6f1-4b81-b8df-c0da46c6c15e"
   },
   "outputs": [],
   "source": [
    "## How many labels were correctly predicted after fine-tuning\n",
    "print(sum(df2.labels == df2.preds_norm),' / 500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUvIfHgURJFg"
   },
   "source": [
    "# Image captioning\n",
    "\n",
    "**NOTE**: You must reload the IDEFICS model before proceeding in this section, if you have fine-tuned the loaded model in the above image classification task. Please restart the kernel, rerun the code in sections \"Installation and Imports\" and \"Load the IDEFICS model with QLoRA\", and then proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYNbaVYeT7au"
   },
   "source": [
    "## Zero-shot sports captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "Jpe-zbMNXK5p",
    "outputId": "b56e8d79-0bb1-4cff-ad47-1a6f97742e7d"
   },
   "outputs": [],
   "source": [
    "image = Image.open('/content/sports-classification/train/football/029.jpg')\n",
    "prompts = [\n",
    "    image,\n",
    "    \"Question: What is a caption for this photo? Answer:\",\n",
    "]\n",
    "display(image)\n",
    "print(check_inference(model, processor, prompts, max_new_tokens=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e05Jef_mY0Fl"
   },
   "source": [
    "## In-context sports captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "1WSV3zgXX30t",
    "outputId": "7facfa32-d134-4c55-d2ff-3bcc7e92c5c5"
   },
   "outputs": [],
   "source": [
    "image_dict = {\n",
    "    './sports-classification/train/axe_throwing/005.jpg':'A man prepares to throw an ax at a target.</s>\\n',\n",
    "    './sports-classification/train/bowling/002.jpg':'A woman rolls a bowling ball down a bowling alley.</s>\\n',\n",
    "    './sports-classification/train/hurdles/014.jpg':'Several competitors jump over hurdles during a race.</s>\\n',\n",
    "    './sports-classification/train/rugby/003.jpg': 'A man in a red jersey tackles a man carrying a rugby ball in a blue jersey.</s>\\n',\n",
    "    './sports-classification/train/football/029.jpg': ''\n",
    "}\n",
    "\n",
    "prompts = []\n",
    "for k in image_dict.keys():\n",
    "    image = Image.open(k)\n",
    "    prompts.append(image)\n",
    "    prompts.append(f\"Question: What is a caption for this photo? Answer: {image_dict[k]}\")\n",
    "\n",
    "print(check_inference(model, processor, prompts, max_new_tokens=15))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyt2Gsxddjf9"
   },
   "source": [
    "## Fine-tune with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpKKmbHygqx5",
    "outputId": "7e9c568d-b29c-458b-b046-7d881feba1d0"
   },
   "outputs": [],
   "source": [
    "## Download the sports subset of the Flickr30k dataset.\n",
    "\n",
    "%cd /content/\n",
    "!git clone https://github.com/ShinThant3010/Captioning-on-Sport-Images.git\n",
    "\n",
    "%cd /content/Captioning-on-Sport-Images/\n",
    "!unzip Training\\ Images.zip\n",
    "!unzip Testing\\ Images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEPGRrByX3ii"
   },
   "outputs": [],
   "source": [
    "## Format as a Huggingface dataset for fine-tuning\n",
    "\n",
    "def process_data(batch):\n",
    "    prompts = []\n",
    "    for i in range(len(batch['image_name'])):\n",
    "        f, l = batch['image_name'][i], batch['comment'][i]\n",
    "        # We split the captions to avoid having very long examples, which would require more GPU ram during training\n",
    "        image = Image.open(f)\n",
    "        prompts.append(\n",
    "            [\n",
    "                image,\n",
    "                f\"Question: What is a caption for this photo? Answer: {l} \",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    inputs = processor(prompts, return_tensors=\"pt\").to(device)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "## Read and format the data\n",
    "train_df = pd.read_csv('/content/Captioning-on-Sport-Images/Overall_Training_Captions_csv.csv',delimiter='|')\n",
    "train_df['image_name'] = '/content/Captioning-on-Sport-Images/Training Images/'+train_df['image_name']\n",
    "train_df.columns = ['image_name', 'comment_number', 'comment']\n",
    "\n",
    "train_split, valid_split = train_test_split(train_df[['image_name']].drop_duplicates(),test_size=0.05,random_state=42)\n",
    "valid_df = train_df[train_df.image_name.isin(valid_split.image_name)]\n",
    "train_df = train_df[train_df.image_name.isin(train_split.image_name)]\n",
    "\n",
    "test_df = pd.read_csv('/content/Captioning-on-Sport-Images/Overall_Training_Captions_csv.csv',delimiter='|')\n",
    "test_df['image_name'] = '/content/Captioning-on-Sport-Images/Testing Images/'+test_df['image_name']\n",
    "test_df.columns = ['image_name', 'comment_number', 'comment']\n",
    "\n",
    "## Create HF dataset and apply transformation\n",
    "captions_dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df.sample(frac=1,random_state=7)),\n",
    "    'valid': Dataset.from_pandas(valid_df.sample(frac=1,random_state=13)),\n",
    "    'test': Dataset.from_pandas(test_df.sample(frac=1,random_state=15)),\n",
    "})\n",
    "\n",
    "captions_dataset['train'].set_transform(process_data)\n",
    "captions_dataset['valid'].set_transform(process_data)\n",
    "captions_dataset['test'].set_transform(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrOOXHBekGkF",
    "outputId": "9fa459fb-821c-4e7e-dc53-4f5af5ece861"
   },
   "outputs": [],
   "source": [
    "captions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V05vYiasfuTa",
    "outputId": "90c62ba2-d370-4fea-f55d-c6877d293401"
   },
   "outputs": [],
   "source": [
    "## Reload IDEFICS and Add the LoRA component to the model\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[1]\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7urKsf2fuQl",
    "outputId": "d85c6e7e-979b-408c-a264-081ec0c238b2"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-captions_3e5\",\n",
    "    learning_rate=3e-5,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=20,\n",
    "    max_steps=200,\n",
    "    num_train_epochs=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=captions_dataset['train'],\n",
    "    eval_dataset=captions_dataset['valid'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "hCu1XXQ3x4wX",
    "outputId": "41ff1247-e0a6-4d2c-b602-eb37fa4adc35"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThlbXPkHx_VT"
   },
   "source": [
    "## Caption with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "G2621pEFfuKw",
    "outputId": "e2b130fc-0ce5-439e-9e73-60c04864b9e6"
   },
   "outputs": [],
   "source": [
    "## Test a single example\n",
    "image = Image.open('/content/sports-classification/train/football/029.jpg')\n",
    "prompts = [\n",
    "    image,\n",
    "    \"Question: What is a caption for this photo? Answer:\",\n",
    "]\n",
    "print(check_inference(trainer.model, processor, prompts))\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06b706b02e9b4815b821a428d46f3b1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "266b1b9893a0492e825841d6453d3a33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e8c929b279148dca4a0a8ac4d420bed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "477f4aa8712449fc911bf1febf50616a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5613366936064871a05bb0761ea84193": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88184606f0354c69989df4c8c6b2173e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e8c929b279148dca4a0a8ac4d420bed",
      "placeholder": "​",
      "style": "IPY_MODEL_ee253a22120b42abb2db14978d6b70b1",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "9d5dd69ff225431a921daad46e7d3078": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a936eb6dde0345e083dab5afb712aa00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06b706b02e9b4815b821a428d46f3b1c",
      "max": 19,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_477f4aa8712449fc911bf1febf50616a",
      "value": 19
     }
    },
    "c9231fcbe0ef4929ad14d552cd0da13d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_88184606f0354c69989df4c8c6b2173e",
       "IPY_MODEL_a936eb6dde0345e083dab5afb712aa00",
       "IPY_MODEL_cbefb02bb1f54d3ba09931230546e6d2"
      ],
      "layout": "IPY_MODEL_5613366936064871a05bb0761ea84193"
     }
    },
    "cbefb02bb1f54d3ba09931230546e6d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d5dd69ff225431a921daad46e7d3078",
      "placeholder": "​",
      "style": "IPY_MODEL_266b1b9893a0492e825841d6453d3a33",
      "value": " 19/19 [03:25&lt;00:00,  8.80s/it]"
     }
    },
    "ee253a22120b42abb2db14978d6b70b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
